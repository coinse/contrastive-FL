{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project to be evaluated\n",
    "pr_type = ['Chart', 'Math', 'Time', 'Lang']\n",
    "project_title = pr_type[0]\n",
    "#pr_version = '1'\n",
    "#project_name = project_title+'_'+pr_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "cur = \"c:/Users/COINSE/Downloads/simfl-extension\"\n",
    "os.chdir(cur)\n",
    "os.chdir('d4j_data')\n",
    "base = os.getcwd()\n",
    "list_project = os.listdir()\n",
    "os.chdir(cur)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n",
      "12412\n",
      "3925263\n",
      "194787\n",
      "9386760\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "# blocked code for data categorization\n",
    "X_pos = {}\n",
    "X_neg = {}\n",
    "dist = []\n",
    "method_list = set()\n",
    "test_list = set()\n",
    "method_test_set = set()\n",
    "bf = 0\n",
    "bp = 0\n",
    "nb = 0\n",
    "for project_name in list_project:\n",
    "    if project_name == 'Math_38' or project_name == 'Math_6':\n",
    "        continue\n",
    "    if project_title in project_name:\n",
    "        os.chdir(f'd4j_data_fix/{project_name}')\n",
    "        with open('mutant_data_new.pkl', 'rb') as mf:\n",
    "            mutant = pickle.load(mf)\n",
    "        with open('test_data.pkl', 'rb') as tf:\n",
    "            test = pickle.load(tf)\n",
    "        os.chdir(cur)\n",
    "        for mutant_no in mutant:\n",
    "            for t in test:\n",
    "                label = 0\n",
    "                if t in mutant[mutant_no]['killer']:\n",
    "                    label = 1\n",
    "                    X_pos[(project_name, mutant_no, t)] = (mutant[mutant_no]['embedding'], test[t], label)\n",
    "                else:\n",
    "                    X_neg[(project_name, mutant_no, t)] = (mutant[mutant_no]['embedding'], test[t], label)\n",
    "                dist.append(np.linalg.norm(mutant[mutant_no]['embedding']-test[t]))\n",
    "                test_list.add(t)\n",
    "                method_list.add(mutant[mutant_no]['method_name'])    \n",
    "                method_test_set.add((mutant[mutant_no]['method_name'], t))\n",
    "                #if label == 1:\n",
    "                #    bf +=1\n",
    "                #elif mutant[mutant_no]['tag'] == 'nb':\n",
    "                #    nb +=1\n",
    "                #else:\n",
    "                #    bp +=1\n",
    "dist = sorted(dist)\n",
    "positive_sample_len = len(X_pos)\n",
    "print(len(test_list))\n",
    "print(len(method_list))\n",
    "print(len(method_test_set))\n",
    "print(len(X_pos))\n",
    "print(len(X_neg))\n",
    "#print(bf, bp, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_modelloss import ContrastiveModel, ContrastiveLoss\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.629128932952881\n",
      "17.977968\n",
      "21.960398\n"
     ]
    }
   ],
   "source": [
    "#config\n",
    "arc = 'relu'\n",
    "mode = 'euclidean'\n",
    "batch_size = 4096\n",
    "num_epoch = 1000\n",
    "expected_epoch = 100\n",
    "projection_dim = 768\n",
    "output_dim = 768\n",
    "init_scale = 0.75\n",
    "final_scale = 0.9\n",
    "dist = sorted(dist)\n",
    "init_margin = dist[int(init_scale*len(dist))]\n",
    "#final_margin = dist[int(final_scale*len(dist))]\n",
    "final_margin = dist[-1]\n",
    "threshold = dist[0] / 2\n",
    "learning_rate = 1e-3\n",
    "res_weight = 1.0\n",
    "print(threshold)\n",
    "print(init_margin)\n",
    "print(final_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "class CustomSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.half_batch = batch_size // 2\n",
    "\n",
    "        self.positive_indices = [i for i, label in enumerate(dataset.labels) if label == 1]\n",
    "        self.negative_indices = [i for i, label in enumerate(dataset.labels) if label == 0]\n",
    "\n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.positive_indices)\n",
    "        random.shuffle(self.negative_indices)\n",
    "\n",
    "        num_batches = min(len(self.positive_indices), len(self.negative_indices)) // self.half_batch\n",
    "\n",
    "        for _ in range(num_batches):\n",
    "            pos_batch = random.sample(self.positive_indices, self.half_batch)\n",
    "            neg_batch = random.sample(self.negative_indices, self.half_batch)\n",
    "\n",
    "            batch_indices = pos_batch + neg_batch\n",
    "            random.shuffle(batch_indices)\n",
    "\n",
    "            yield batch_indices\n",
    "    def __len__(self):\n",
    "        return min(len(self.positive_indices), len(self.negative_indices)) // self.half_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\COINSE\\Downloads\\simfl-extension\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 trained with 194787 data, average loss:8.085884150117636, Gradient_norm:0.6748452364588247\n",
      "epoch 2 trained with 194787 data, average loss:0.3682563106218974, Gradient_norm:0.29788099595775913\n",
      "epoch 3 trained with 194787 data, average loss:0.21346924485017857, Gradient_norm:0.21098569342793153\n",
      "epoch 4 trained with 194787 data, average loss:0.14546528846646348, Gradient_norm:0.15806058773048912\n",
      "epoch 5 trained with 194787 data, average loss:0.10676086818178494, Gradient_norm:0.11813906802380454\n",
      "epoch 6 trained with 194787 data, average loss:0.08246173756197095, Gradient_norm:0.11926199438431312\n",
      "epoch 7 trained with 194787 data, average loss:0.0659537947891901, Gradient_norm:0.09456174508625062\n",
      "epoch 8 trained with 194787 data, average loss:0.05415359317945937, Gradient_norm:0.08232993761917858\n",
      "epoch 9 trained with 194787 data, average loss:0.04539952566847205, Gradient_norm:0.07102939317925543\n",
      "epoch 10 trained with 194787 data, average loss:0.03873415167133013, Gradient_norm:0.06600665652959367\n",
      "epoch 11 trained with 194787 data, average loss:0.033498829851547875, Gradient_norm:0.062101335539783406\n",
      "epoch 12 trained with 194787 data, average loss:0.029291022297305364, Gradient_norm:0.053530522951980915\n",
      "epoch 13 trained with 194787 data, average loss:0.025868865894153714, Gradient_norm:0.05344392138747093\n",
      "epoch 14 trained with 194787 data, average loss:0.023039657312134903, Gradient_norm:0.050046188039943054\n",
      "epoch 15 trained with 194787 data, average loss:0.020638882609394688, Gradient_norm:0.04366346881414265\n",
      "epoch 16 trained with 194787 data, average loss:0.018619817254754405, Gradient_norm:0.041988515936809225\n",
      "epoch 17 trained with 194787 data, average loss:0.01688054398012658, Gradient_norm:0.03660466329865183\n",
      "epoch 18 trained with 194787 data, average loss:0.015385837701614946, Gradient_norm:0.036705600202233714\n",
      "epoch 19 trained with 194787 data, average loss:0.014067830964146802, Gradient_norm:0.034228987883857184\n",
      "epoch 20 trained with 194787 data, average loss:0.012928104513169577, Gradient_norm:0.03327284359729323\n",
      "epoch 21 trained with 194787 data, average loss:0.01191369395625467, Gradient_norm:0.029233911922883167\n",
      "epoch 22 trained with 194787 data, average loss:0.011020699496536205, Gradient_norm:0.030719826765800658\n",
      "epoch 23 trained with 194787 data, average loss:0.010219253735461583, Gradient_norm:0.029703426391291997\n",
      "epoch 24 trained with 194787 data, average loss:0.009495772537775338, Gradient_norm:0.027495688159138817\n",
      "epoch 25 trained with 194787 data, average loss:0.008842020179145038, Gradient_norm:0.025333424410135877\n",
      "epoch 26 trained with 194787 data, average loss:0.008259066943234453, Gradient_norm:0.024548208484417834\n",
      "epoch 27 trained with 194787 data, average loss:0.007727329396099473, Gradient_norm:0.024828253086834076\n",
      "epoch 28 trained with 194787 data, average loss:0.007246451772516593, Gradient_norm:0.024487524065020944\n",
      "epoch 29 trained with 194787 data, average loss:0.0068032002115311725, Gradient_norm:0.022714840083205446\n",
      "epoch 30 trained with 194787 data, average loss:0.0063970508247924345, Gradient_norm:0.02276745569894742\n",
      "epoch 31 trained with 194787 data, average loss:0.006024879626541709, Gradient_norm:0.02160923134178895\n",
      "epoch 32 trained with 194787 data, average loss:0.005682240851456299, Gradient_norm:0.020590601459430823\n",
      "epoch 33 trained with 194787 data, average loss:0.005365214029249425, Gradient_norm:0.020732834876000052\n",
      "epoch 34 trained with 194787 data, average loss:0.0050727464743734645, Gradient_norm:0.01970315413116655\n",
      "epoch 35 trained with 194787 data, average loss:0.004795633557174976, Gradient_norm:0.01829379395632441\n",
      "epoch 36 trained with 194787 data, average loss:0.004549261570597689, Gradient_norm:0.01798417833771169\n",
      "epoch 37 trained with 194787 data, average loss:0.004313877618794019, Gradient_norm:0.017500994941010668\n",
      "epoch 38 trained with 194787 data, average loss:0.004093861304378758, Gradient_norm:0.016159146381035244\n",
      "epoch 39 trained with 194787 data, average loss:0.003892381001302662, Gradient_norm:0.01687309909403761\n",
      "epoch 40 trained with 194787 data, average loss:0.003701612624960641, Gradient_norm:0.01618852433847011\n",
      "epoch 41 trained with 194787 data, average loss:0.0035261260588110113, Gradient_norm:0.01617901778824579\n",
      "epoch 42 trained with 194787 data, average loss:0.003356952986602361, Gradient_norm:0.016060754604061315\n",
      "epoch 43 trained with 194787 data, average loss:0.003201952888048254, Gradient_norm:0.014849975083928966\n",
      "epoch 44 trained with 194787 data, average loss:0.0030557764548575506, Gradient_norm:0.014920442559796816\n",
      "epoch 45 trained with 194787 data, average loss:0.002917222329415381, Gradient_norm:0.014726048933674034\n",
      "epoch 46 trained with 194787 data, average loss:0.00278603911283426, Gradient_norm:0.013812220198388717\n",
      "epoch 47 trained with 194787 data, average loss:0.0026636579423211515, Gradient_norm:0.013346476461180685\n",
      "epoch 48 trained with 194787 data, average loss:0.0025470418186159804, Gradient_norm:0.013408639451650342\n",
      "epoch 49 trained with 194787 data, average loss:0.0024366173650681353, Gradient_norm:0.013422517583978876\n",
      "epoch 50 trained with 194787 data, average loss:0.002334420093878483, Gradient_norm:0.012952571653275473\n",
      "epoch 51 trained with 194787 data, average loss:0.002234322547640962, Gradient_norm:0.01194348752865134\n",
      "epoch 52 trained with 194787 data, average loss:0.002141148885129951, Gradient_norm:0.011444216301210697\n",
      "epoch 53 trained with 194787 data, average loss:0.0020536364609142765, Gradient_norm:0.011469738575183689\n",
      "epoch 54 trained with 194787 data, average loss:0.0019702256249729544, Gradient_norm:0.011371200472371822\n",
      "epoch 55 trained with 194787 data, average loss:0.001892138924934746, Gradient_norm:0.012271150769745966\n",
      "epoch 56 trained with 194787 data, average loss:0.0018151036371515754, Gradient_norm:0.010834081302517687\n",
      "epoch 57 trained with 194787 data, average loss:0.001743567394441925, Gradient_norm:0.011640071041664157\n",
      "epoch 58 trained with 194787 data, average loss:0.0016731794263857107, Gradient_norm:0.010131608734384128\n",
      "epoch 59 trained with 194787 data, average loss:0.0016094393940875307, Gradient_norm:0.010398202879999624\n",
      "epoch 60 trained with 194787 data, average loss:0.0015467828052351251, Gradient_norm:0.009991479014154708\n",
      "epoch 61 trained with 194787 data, average loss:0.0014867479136834543, Gradient_norm:0.009558561741711697\n",
      "epoch 62 trained with 194787 data, average loss:0.0014308543225827937, Gradient_norm:0.009622519161098138\n",
      "epoch 63 trained with 194787 data, average loss:0.0013776891064480878, Gradient_norm:0.009885958530115584\n",
      "epoch 64 trained with 194787 data, average loss:0.001325505851127673, Gradient_norm:0.009652311296079213\n",
      "epoch 65 trained with 194787 data, average loss:0.0012761895486619323, Gradient_norm:0.009081586031042643\n",
      "epoch 66 trained with 194787 data, average loss:0.0012293167043632518, Gradient_norm:0.009513804656627732\n",
      "epoch 67 trained with 194787 data, average loss:0.001184046234508666, Gradient_norm:0.008923815061256562\n",
      "epoch 68 trained with 194787 data, average loss:0.001140842088110124, Gradient_norm:0.009209443083774955\n",
      "epoch 69 trained with 194787 data, average loss:0.0010998626239597797, Gradient_norm:0.009088991950047347\n",
      "epoch 70 trained with 194787 data, average loss:0.0010600475555596252, Gradient_norm:0.00851185687613518\n",
      "epoch 71 trained with 194787 data, average loss:0.001022133874357678, Gradient_norm:0.008490695917868175\n",
      "epoch 72 trained with 194787 data, average loss:0.000985813646063131, Gradient_norm:0.00827672817254733\n",
      "epoch 73 trained with 194787 data, average loss:0.0009507003378530499, Gradient_norm:0.008272727498721773\n",
      "epoch 74 trained with 194787 data, average loss:0.0009170409502985422, Gradient_norm:0.007715238045733548\n",
      "epoch 75 trained with 194787 data, average loss:0.0008846476060474137, Gradient_norm:0.007708376557962176\n",
      "epoch 76 trained with 194787 data, average loss:0.0008538324497446107, Gradient_norm:0.008137620234221784\n",
      "epoch 77 trained with 194787 data, average loss:0.0008240379441607123, Gradient_norm:0.0077918921113748095\n",
      "epoch 78 trained with 194787 data, average loss:0.000795667924345859, Gradient_norm:0.007109868921828715\n",
      "epoch 79 trained with 194787 data, average loss:0.000767876249786544, Gradient_norm:0.007100947615489489\n",
      "epoch 80 trained with 194787 data, average loss:0.0007415772415697575, Gradient_norm:0.007155638699638838\n",
      "epoch 81 trained with 194787 data, average loss:0.0007164804956119042, Gradient_norm:0.007074517344875739\n",
      "epoch 82 trained with 194787 data, average loss:0.0006922787797520868, Gradient_norm:0.007315886564000158\n",
      "epoch 83 trained with 194787 data, average loss:0.0006685616353934165, Gradient_norm:0.007156767182472966\n",
      "epoch 84 trained with 194787 data, average loss:0.0006460142467403784, Gradient_norm:0.007044768336115147\n",
      "epoch 85 trained with 194787 data, average loss:0.0006247610059896639, Gradient_norm:0.007398909594471644\n",
      "epoch 86 trained with 194787 data, average loss:0.0006036812543849616, Gradient_norm:0.006931774930556411\n",
      "epoch 87 trained with 194787 data, average loss:0.0005833247184151938, Gradient_norm:0.006854586601913136\n",
      "epoch 88 trained with 194787 data, average loss:0.0005637530618211409, Gradient_norm:0.006809380506851797\n",
      "epoch 89 trained with 194787 data, average loss:0.0005453025017535159, Gradient_norm:0.006908295844306628\n",
      "epoch 90 trained with 194787 data, average loss:0.0005270585218871323, Gradient_norm:0.006240370552198877\n",
      "epoch 91 trained with 194787 data, average loss:0.000510064064656035, Gradient_norm:0.006172869149272957\n",
      "epoch 92 trained with 194787 data, average loss:0.0004936259520036401, Gradient_norm:0.006491378289709827\n",
      "epoch 93 trained with 194787 data, average loss:0.0004769608373559701, Gradient_norm:0.006060787472451384\n",
      "epoch 94 trained with 194787 data, average loss:0.0004618413167918334, Gradient_norm:0.00610229298334922\n",
      "epoch 95 trained with 194787 data, average loss:0.0004461329851134603, Gradient_norm:0.006065536119452147\n",
      "epoch 96 trained with 194787 data, average loss:0.0004316668261405236, Gradient_norm:0.005493396587030857\n",
      "epoch 97 trained with 194787 data, average loss:0.00041774321895597194, Gradient_norm:0.0058101586010626365\n",
      "epoch 98 trained with 194787 data, average loss:0.0004035750249992513, Gradient_norm:0.0055061048607833426\n",
      "epoch 99 trained with 194787 data, average loss:0.0003904127139927975, Gradient_norm:0.005213740920724656\n",
      "epoch 100 trained with 194787 data, average loss:0.00037815757908295683, Gradient_norm:0.0055094247831993575\n",
      "epoch 101 trained with 194787 data, average loss:0.00036598724424644996, Gradient_norm:0.005716914552521485\n",
      "epoch 102 trained with 194787 data, average loss:0.00035442264985855826, Gradient_norm:0.005648902750524199\n",
      "epoch 103 trained with 194787 data, average loss:0.00034291499529596575, Gradient_norm:0.005356577324156188\n",
      "epoch 104 trained with 194787 data, average loss:0.0003331734472643196, Gradient_norm:0.005279126799527581\n",
      "epoch 105 trained with 194787 data, average loss:0.00032225878264095326, Gradient_norm:0.005707774846738686\n",
      "epoch 106 trained with 194787 data, average loss:0.00031137585028773174, Gradient_norm:0.005451926559296432\n",
      "epoch 107 trained with 194787 data, average loss:0.0003011632785880162, Gradient_norm:0.005356615229725152\n",
      "epoch 108 trained with 194787 data, average loss:0.0002915304939961061, Gradient_norm:0.0051496100811164\n",
      "epoch 109 trained with 194787 data, average loss:0.0002822999128208418, Gradient_norm:0.005268622654422867\n",
      "epoch 110 trained with 194787 data, average loss:0.00027304290748967713, Gradient_norm:0.004800305416679986\n",
      "epoch 111 trained with 194787 data, average loss:0.00026472937073170516, Gradient_norm:0.004819970706209472\n",
      "epoch 112 trained with 194787 data, average loss:0.0002564688587275062, Gradient_norm:0.0052442790037250595\n",
      "epoch 113 trained with 194787 data, average loss:0.00024795643366815057, Gradient_norm:0.005144729203980378\n",
      "epoch 114 trained with 194787 data, average loss:0.00024057631405109228, Gradient_norm:0.00525325781759402\n",
      "epoch 115 trained with 194787 data, average loss:0.00023258220577796843, Gradient_norm:0.004673322096247948\n",
      "epoch 116 trained with 194787 data, average loss:0.00022530366701782137, Gradient_norm:0.004779649060505429\n",
      "epoch 117 trained with 194787 data, average loss:0.0002183127947622173, Gradient_norm:0.004907772805223859\n",
      "epoch 118 trained with 194787 data, average loss:0.00022884486467470802, Gradient_norm:0.007627818690571889\n",
      "epoch 119 trained with 194787 data, average loss:0.00029243508106446825, Gradient_norm:0.009611876635417679\n",
      "epoch 120 trained with 194787 data, average loss:0.00021161700715310872, Gradient_norm:0.005147928494231454\n",
      "epoch 121 trained with 194787 data, average loss:0.0001922435061108748, Gradient_norm:0.005151172944866428\n",
      "epoch 122 trained with 194787 data, average loss:0.0001859328128072472, Gradient_norm:0.004483742462939503\n",
      "epoch 123 trained with 194787 data, average loss:0.00018012381375835199, Gradient_norm:0.005206304141776305\n",
      "epoch 124 trained with 194787 data, average loss:0.00017463059884903487, Gradient_norm:0.005038318171417425\n",
      "epoch 125 trained with 194787 data, average loss:0.00016919836283098752, Gradient_norm:0.005564244199723117\n",
      "epoch 126 trained with 194787 data, average loss:0.00016380547185690375, Gradient_norm:0.00524666841763512\n",
      "epoch 127 trained with 194787 data, average loss:0.0001593618823487001, Gradient_norm:0.005852352975445512\n",
      "epoch 128 trained with 194787 data, average loss:0.00015432639944871576, Gradient_norm:0.0048843800294107505\n",
      "epoch 129 trained with 194787 data, average loss:0.00014926887221614985, Gradient_norm:0.00707909592842661\n",
      "epoch 130 trained with 194787 data, average loss:0.00017306674180872506, Gradient_norm:0.02645256766459497\n",
      "epoch 131 trained with 194787 data, average loss:0.00015349831422402835, Gradient_norm:0.012595391908148368\n",
      "epoch 132 trained with 194787 data, average loss:0.0001495747822749157, Gradient_norm:0.016850684486770042\n",
      "epoch 133 trained with 194787 data, average loss:0.00025555370666552335, Gradient_norm:0.04266507338431824\n",
      "epoch 134 trained with 194787 data, average loss:0.00014539984401077768, Gradient_norm:0.008416037033759548\n",
      "epoch 135 trained with 194787 data, average loss:0.00016083720553676054, Gradient_norm:0.012932203728545763\n",
      "epoch 136 trained with 194787 data, average loss:0.00013280656685310532, Gradient_norm:0.007522379996045892\n",
      "epoch 137 trained with 194787 data, average loss:0.00019302825148770353, Gradient_norm:0.057800112254039554\n",
      "epoch 138 trained with 194787 data, average loss:0.00016755750023852065, Gradient_norm:0.0122486522473629\n",
      "epoch 139 trained with 194787 data, average loss:0.0001221604409996265, Gradient_norm:0.008271284383950719\n",
      "epoch 140 trained with 194787 data, average loss:0.0001697237371445226, Gradient_norm:0.06626656199816645\n",
      "epoch 141 trained with 194787 data, average loss:0.00027936140835057205, Gradient_norm:0.018068933096922285\n",
      "epoch 142 trained with 194787 data, average loss:0.00010745818493281452, Gradient_norm:0.006787046686558887\n",
      "epoch 143 trained with 194787 data, average loss:0.00010579709396552062, Gradient_norm:0.020915924975691877\n",
      "epoch 144 trained with 194787 data, average loss:0.00012561601033667102, Gradient_norm:0.011514611405943623\n",
      "epoch 145 trained with 194787 data, average loss:0.00010325822753050791, Gradient_norm:0.04548146117120885\n",
      "epoch 146 trained with 194787 data, average loss:0.00015732078145447304, Gradient_norm:0.016865876246895298\n",
      "epoch 147 trained with 194787 data, average loss:0.00010005100936420301, Gradient_norm:0.019914800501092053\n",
      "epoch 148 trained with 194787 data, average loss:0.00022291266441243351, Gradient_norm:0.054035235817072154\n",
      "epoch 149 trained with 194787 data, average loss:0.00015392326986329863, Gradient_norm:0.013378191154889278\n",
      "epoch 150 trained with 194787 data, average loss:9.446255095705662e-05, Gradient_norm:0.015412557158183526\n",
      "epoch 151 trained with 194787 data, average loss:0.0001314024962084659, Gradient_norm:0.01029759496473502\n",
      "epoch 152 trained with 194787 data, average loss:0.00011262815299536062, Gradient_norm:0.0622923962082294\n",
      "epoch 153 trained with 194787 data, average loss:0.00015999159829031365, Gradient_norm:0.016665771796662385\n",
      "epoch 154 trained with 194787 data, average loss:9.103168667934369e-05, Gradient_norm:0.03091452604859643\n",
      "epoch 155 trained with 194787 data, average loss:0.00015408285844387137, Gradient_norm:0.04096520135623381\n",
      "epoch 156 trained with 194787 data, average loss:0.00011681394971674308, Gradient_norm:0.017857608639237015\n",
      "epoch 157 trained with 194787 data, average loss:0.00011938800010587632, Gradient_norm:0.013957177891619939\n",
      "epoch 158 trained with 194787 data, average loss:0.00016809175849630265, Gradient_norm:0.05077579259349078\n",
      "epoch 159 trained with 194787 data, average loss:0.00010721937420991405, Gradient_norm:0.010810794692769831\n",
      "epoch 160 trained with 194787 data, average loss:7.375267477982561e-05, Gradient_norm:0.04001603634661343\n",
      "epoch 161 trained with 194787 data, average loss:0.00017941844801801685, Gradient_norm:0.013842115116172497\n",
      "epoch 162 trained with 194787 data, average loss:7.891147788541275e-05, Gradient_norm:0.015107117366858418\n",
      "epoch 163 trained with 194787 data, average loss:0.00010987386137154924, Gradient_norm:0.02284857274800319\n",
      "epoch 164 trained with 194787 data, average loss:8.011159828432331e-05, Gradient_norm:0.016541654363170916\n",
      "epoch 165 trained with 194787 data, average loss:0.000176733926612845, Gradient_norm:0.027621243682894172\n",
      "epoch 166 trained with 194787 data, average loss:7.68090801557264e-05, Gradient_norm:0.01103192370537092\n",
      "epoch 167 trained with 194787 data, average loss:7.648846152126983e-05, Gradient_norm:0.018510644985046194\n",
      "epoch 168 trained with 194787 data, average loss:9.987131246210386e-05, Gradient_norm:0.015932718593045632\n",
      "epoch 169 trained with 194787 data, average loss:9.290527297404576e-05, Gradient_norm:0.01931019023592256\n",
      "epoch 170 trained with 194787 data, average loss:0.00012966817515310444, Gradient_norm:0.01807238063668733\n"
     ]
    }
   ],
   "source": [
    "#blocked code for using bigger model, progressive margin, and learning rate management (not done during previous result)\n",
    "ds_mod = ['zero']\n",
    "m = 'euclidean'\n",
    "for ds in ds_mod:\n",
    "    X = []\n",
    "    if ds == 'full':\n",
    "        for key_pair in X_pos:\n",
    "            X.append(X_pos[key_pair])\n",
    "        for key_pair in X_neg:\n",
    "            X.append(X_neg[key_pair])\n",
    "    if m == 'cosine':\n",
    "        margin = 1.0\n",
    "        threshold = 0.5\n",
    "    model = ContrastiveModel(embedding_dim=768, projection_dim=projection_dim, output_dim=output_dim, mode=m)\n",
    "    loss = ContrastiveLoss(margin=init_margin)\n",
    "    #model = ContrastiveModel(embedding_dim=768, projection_dim=projection_dim, output_dim=output_dim, mode=m, res_weight=res_weight)\n",
    "    #loss = ContrastiveLoss(init_margin=init_margin, final_margin=final_margin, expected_epoch=expected_epoch)\n",
    "    optimizer = torch.optim.Adam(\n",
    "            params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=learning_rate)\n",
    "    steps_per_epoch = len(X)/batch_size\n",
    "    if ds != 'full':\n",
    "        if ds == 'one':\n",
    "            steps_per_epoch = 2 * positive_sample_len/batch_size\n",
    "        else:\n",
    "            steps_per_epoch = positive_sample_len/batch_size\n",
    "    total_steps = steps_per_epoch * expected_epoch\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    power = 2\n",
    "    def warmup_lr_lambda(current_step: int):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        return 1.0\n",
    "    #warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr_lambda)\n",
    "    #plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6, verbose=True)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    p_counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "    loss_list = []\n",
    "    for epoch in range(num_epoch):\n",
    "        method_test_set = set()\n",
    "        if ds != 'full':\n",
    "            X = []\n",
    "            for key_pair in X_pos:\n",
    "                X.append(X_pos[key_pair])\n",
    "        if ds == 'one':\n",
    "            neg_key_pair = list(X_neg.keys())\n",
    "            random.shuffle(neg_key_pair)\n",
    "            for key_pair in neg_key_pair:\n",
    "                if len(X) < 2*positive_sample_len:\n",
    "                    if (key_pair[1], key_pair[2]) not in method_test_set:\n",
    "                        X.append(X_neg[key_pair])\n",
    "                        method_test_set.add((key_pair[1], key_pair[2]))\n",
    "            X_label = [x[2] for x in X]\n",
    "            X_data = [x[:2] for x in X]\n",
    "            dataset = CustomDataset(X_data, X_label)\n",
    "            sampler = CustomSampler(dataset, batch_size)\n",
    "            train_data = DataLoader(dataset, batch_sampler=sampler)\n",
    "        else:\n",
    "            X_label = [x[2] for x in X]\n",
    "            X_data = [x[:2] for x in X]\n",
    "            dataset = CustomDataset(X_data, X_label)\n",
    "            train_data = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (data, label) in enumerate(train_data):\n",
    "            test = data[1].to(device)\n",
    "            method = data[0].to(device)\n",
    "            label = torch.Tensor(label)\n",
    "            label = label.to(device)\n",
    "            output = model(test, method)\n",
    "            optimizer.zero_grad()\n",
    "            l, _, _ = loss(output, label)\n",
    "            #l, _, _ = loss(output, label, epoch)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            step = steps_per_epoch*epoch+batch_idx\n",
    "            #if step < warmup_steps:\n",
    "            #    warmup_scheduler.step()\n",
    "            epoch_loss += l.item()\n",
    "        grad_norm = compute_gradient_norm(model)\n",
    "        avg_epoch_loss = epoch_loss / len(train_data)\n",
    "        loss_list.append(avg_epoch_loss)\n",
    "        print(f'epoch {epoch+1} trained with {len(X)} data, average loss:{avg_epoch_loss}, Gradient_norm:{grad_norm}')\n",
    "        #if step >= warmup_steps:\n",
    "        #    plateau_scheduler.step(avg_epoch_loss)\n",
    "        if avg_epoch_loss<best_val_loss:\n",
    "            best_val_loss = avg_epoch_loss\n",
    "            p_counter = 0\n",
    "        else:\n",
    "            p_counter+=1\n",
    "        if p_counter >= 10:\n",
    "            if epoch+1>100:\n",
    "                break\n",
    "    epochs = list(range(1, len(loss_list)+1))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, loss_list, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "    # Adding titles and labels\n",
    "    plt.title('Training Loss Over Epochs', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=12)\n",
    "    os.makedirs(f'results/{project_title}', exist_ok=True)\n",
    "    plt.savefig(f'results/{project_title}/{arc}_{ds}_newloss.png', format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    os.makedirs(f'new-model/{project_title}', exist_ok=True)\n",
    "    torch.save(model.state_dict(), f'new-model/{project_title}/model_{arc}_{ds}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
