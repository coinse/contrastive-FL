{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project to be evaluated\n",
    "pr_type = ['Chart', 'Math', 'Time', 'Lang']\n",
    "project_title = pr_type[0]\n",
    "#pr_version = '1'\n",
    "#project_name = project_title+'_'+pr_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "cur = \"c:/Users/COINSE/Downloads/simfl-extension\"\n",
    "os.chdir(cur)\n",
    "os.chdir('d4j_data')\n",
    "base = os.getcwd()\n",
    "list_project = os.listdir()\n",
    "os.chdir(cur)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function from simfl-source\n",
    "def get_failing_tests(project, fault_no, ftc_path):\n",
    "    file_path = os.path.join(ftc_path, project, str(fault_no))\n",
    "    ftc = []\n",
    "    with open(file_path, \"r\") as ft_file:\n",
    "        for test_case in ft_file:\n",
    "            ftc.append(test_case.strip())\n",
    "    return ftc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32533\n",
      "4096.0\n",
      "4096 4096\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "dist = []\n",
    "batches = []\n",
    "x = 0\n",
    "mn = float('inf')\n",
    "mx = 0\n",
    "flag = True\n",
    "for project_name in list_project:\n",
    "    if project_name == 'Math_38' or project_name == 'Math_6':\n",
    "        continue\n",
    "    if project_title in project_name:\n",
    "        project = project_name.split('_')[0]\n",
    "        project_version = project_name.split('_')[1]\n",
    "        FT_PATH = \"./failing_tests\"\n",
    "        FAILING_TESTS = get_failing_tests(project, project_version, FT_PATH)\n",
    "        os.chdir(f'd4j_data_fix/{project_name}')\n",
    "        with open('mutant_data_new.pkl', 'rb') as mf:\n",
    "            mutant = pickle.load(mf)\n",
    "        with open('test_data_new.pkl', 'rb') as tf:\n",
    "            test = pickle.load(tf)\n",
    "        with open('method_data_new.pkl', 'rb') as mef:\n",
    "            method = pickle.load(mef)\n",
    "        os.chdir(cur)\n",
    "        for mutant_no in mutant:\n",
    "            if mutant[mutant_no]['killer']:\n",
    "                dp = []\n",
    "                ct = None\n",
    "                ctd = float('inf')\n",
    "                for t in mutant[mutant_no]['killer']:\n",
    "                    d = np.linalg.norm(mutant[mutant_no]['embedding'] - test[t])\n",
    "                    if d < ctd:\n",
    "                        ctd = d\n",
    "                        ct = t\n",
    "                dp.append((mutant[mutant_no]['embedding'], test[ct], 1))\n",
    "                for m in method:\n",
    "                    if method[m]['method_name'] != mutant[mutant_no]['method_name']:\n",
    "                        dp.append((method[m]['embedding'], test[ct], 0))\n",
    "                        dist.append(np.linalg.norm(method[m]['embedding']- test[ct]))\n",
    "                    if len(dp)>= 4096:\n",
    "                        break\n",
    "                random.shuffle(dp)\n",
    "                batches.append(dp)\n",
    "                x += len(dp)\n",
    "                if len(dp)>mx:\n",
    "                    mx = len(dp)\n",
    "                if len(dp)<mn:\n",
    "                    mn = len(dp)\n",
    "                if flag:\n",
    "                    print(sys.getsizeof(dp))\n",
    "print(len(batches))\n",
    "print(x / len(batches))\n",
    "print(mn, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from version_batch_modelloss import ContrastiveModel, ContrastiveLoss\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "import matplotlib.pyplot as plt\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedBatchDataset(Dataset):\n",
    "    def __init__(self, batches):\n",
    "        self.batches = batches  # List of precomputed batches\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)  # Number of batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.batches[idx]  # Return batch directly\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Optimized collate function for DataLoader\"\"\"\n",
    "    method_batch = torch.stack([torch.from_numpy(x[0]) for x in batch], dim=0)\n",
    "    test_batch = torch.stack([torch.from_numpy(x[1]) for x in batch], dim=0)\n",
    "    label = torch.tensor([x[2] for x in batch], dtype=torch.float)\n",
    "    \n",
    "    return method_batch.pin_memory(), test_batch.pin_memory(), label.pin_memory()\n",
    "\n",
    "\n",
    "dataset = PrecomputedBatchDataset(batches)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4, persistent_workers=True, pin_memory=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3949568271636963\n",
      "17.801548\n",
      "21.57538\n"
     ]
    }
   ],
   "source": [
    "#config\n",
    "num_epoch = 1000\n",
    "expected_epoch = 100\n",
    "projection_dim = 768\n",
    "output_dim = 768\n",
    "init_scale = 0.75\n",
    "final_scale = 0.9\n",
    "dist = sorted(dist)\n",
    "init_margin = dist[int(init_scale*len(dist))]\n",
    "final_margin = dist[-1]\n",
    "threshold = dist[0] / 2\n",
    "learning_rate = 1e-3\n",
    "res_weight = 1.0\n",
    "print(threshold)\n",
    "print(init_margin)\n",
    "print(final_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\COINSE\\Downloads\\simfl-extension\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\COINSE\\Downloads\\simfl-extension\\myenv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start\n"
     ]
    }
   ],
   "source": [
    "#blocked code for using bigger model, progressive margin, and learning rate management (not done during previous result)\n",
    "arc = 'leaky_relu'\n",
    "a = 0.1\n",
    "m = 'euclidean'\n",
    "model = ContrastiveModel(embedding_dim=768, projection_dim=projection_dim, output_dim=output_dim, mode=m)\n",
    "loss = ContrastiveLoss(margin=init_margin)\n",
    "optimizer = torch.optim.Adam(\n",
    "        params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate)\n",
    "steps_per_epoch = len(batches)\n",
    "total_steps = steps_per_epoch * expected_epoch\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "power = 2\n",
    "def warmup_lr_lambda(current_step: int):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr_lambda)\n",
    "plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6, verbose=True)\n",
    "model.to(device)\n",
    "model.train()\n",
    "p_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "loss_list = []\n",
    "positive_loss_list = []\n",
    "negative_loss_list = []\n",
    "print('training start')\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    positive_epoch_loss = 0.0\n",
    "    negative_epoch_loss = 0.0\n",
    "    for batch_idx, (method_batch, test_batch, label) in tqdm(enumerate(dataloader)):\n",
    "        method_batch = method_batch.to(device, non_blocking=True)\n",
    "        test_batch = test_batch.to(device, non_blocking=True)\n",
    "        label = label.to(device, non_blocking=True)\n",
    "\n",
    "        output = model(test_batch, method_batch)\n",
    "    #for batch_idx, dp in tqdm(enumerate(dataloader)):\n",
    "    #    method_batch = torch.stack([torch.from_numpy(x[0]) for x in dp]).to(device)\n",
    "    #    test_batch = torch.stack([torch.from_numpy(x[1]) for x in dp]).to(device)\n",
    "    #    label = torch.stack([torch.tensor(x[2], dtype=torch.float) for x in dp]).to(device)\n",
    "    #    output = model(test_batch, method_batch)\n",
    "        optimizer.zero_grad()\n",
    "        l, pl, nl = loss(output, label)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        step = steps_per_epoch*epoch+batch_idx\n",
    "        if step < warmup_steps:\n",
    "            warmup_scheduler.step()\n",
    "        epoch_loss += l.item()\n",
    "        positive_epoch_loss += pl.item()\n",
    "        negative_epoch_loss += nl.item()\n",
    "    grad_norm = compute_gradient_norm(model)\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(batches)\n",
    "    positive_avg_epoch_loss = positive_epoch_loss / len(batches)\n",
    "    negative_avg_epoch_loss = negative_epoch_loss / len(batches)\n",
    "    \n",
    "    loss_list.append(avg_epoch_loss)\n",
    "    positive_loss_list.append(positive_avg_epoch_loss)\n",
    "    negative_loss_list.append(negative_avg_epoch_loss)\n",
    "\n",
    "    print(f'epoch {epoch+1} trained with {x} data, average loss:{avg_epoch_loss}, Gradient_norm:{grad_norm}')\n",
    "    if step >= warmup_steps:\n",
    "        plateau_scheduler.step(avg_epoch_loss)\n",
    "    if avg_epoch_loss<best_val_loss:\n",
    "        best_val_loss = avg_epoch_loss\n",
    "        p_counter = 0\n",
    "    else:\n",
    "        p_counter+=1\n",
    "    if p_counter >= 5:\n",
    "        if epoch+1>30:\n",
    "            break\n",
    "    if epoch+1 % 5 == 0:\n",
    "        torch.save(model.state_dict(), f'new-model/{project_title}/version_batch/model_{arc}_{a}_{m}_{epoch+1}.pth')\n",
    "epochs = list(range(1, len(loss_list)+1))\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, loss_list, marker='o', linestyle='-', color='g', label='Training Loss')\n",
    "plt.plot(epochs, positive_loss_list, marker='o', linestyle='-', color='b', label='Positive Training Loss')\n",
    "plt.plot(epochs, negative_loss_list, marker='o', linestyle='-', color='r', label='Negative Training Loss')\n",
    "# Adding titles and labels\n",
    "plt.title('Training Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "os.makedirs(f'CROFL results/version_batch/{project_title}', exist_ok=True)\n",
    "plt.savefig(f'CROFL results/version_batch/{project_title}/{arc}_newloss_{m}.png', format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "os.makedirs(f'new-model/{project_title}', exist_ok=True)\n",
    "torch.save(model.state_dict(), f'new-model/{project_title}/version_batch/model_{arc}_{a}_{m}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
